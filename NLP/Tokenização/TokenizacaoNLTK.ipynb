{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivo:\n",
        "\n",
        "Nesse laboratório, você extrairá um romance de um arquivo HTML retirado do site Projeto Gutenberg (que contém um grande corpus de livros) usando o pacote de requisições do Python. Em seguida, você normalizará e tokenizará o texto. Você também irá analisar a distribuição de palavras. Ao final você deverá simular a execução do algoritmo BPE (Byte-Pair Encoding) e calcular a distância de edição entre duas palavras."
      ],
      "metadata": {
        "id": "AzRN0mmg27MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bibliotecas**"
      ],
      "metadata": {
        "id": "DEgDOZs7qkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "D9tkHpFtqmTD",
        "outputId": "7be43674-f2c7-402e-b666-de02a1b03b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Obtenção dos dados**\n",
        "\n",
        "Para a atividade desse laborátorio, iremos utilizar dados textuais oriundos do **[Project Gutenberg](https://www.gutenberg.org/)**. O texto escolhido é a obra Moby Dick."
      ],
      "metadata": {
        "id": "czDyUObFndhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6eYoitlP77",
        "outputId": "389a3a9c-a0f7-429f-9f10-b595b9999f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\r\n",
            "<html lang=\"en\">\r\n",
            "<head>\r\n",
            "  <meta charset=\"utf-8\">\r\n",
            "<title>Moby Dick; or The Whale</title>\r\n",
            "\r\n",
            "<style>\r\n",
            "\r\n",
            "    body {margin-left:10%; margin-right:10%; text-align:justify }\r\n",
            "    p { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
            "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
            "    hr  { width: 50%; text-align: center;}\r\n",
            "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
            "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
            "    .toc       { margin-left: 10%; margin-bottom: .75em;}\r\n",
            "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\r\n",
            "\r\n",
            "    table      {margin-left: 10%;}\r\n",
            "\r\n",
            "a:link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:visited {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:hover {color:red}\r\n",
            "\r\n",
            "</style>\r\n",
            "  </head>\r\n",
            "  <body>\r\n",
            "<div>*** START OF THE PROJECT GUTENBERG EBOOK 2701 ***</div>\r\n",
            "    <h1>\r\n",
            "      MOBY-DICK;<br><br>or, THE WHALE.\r\n",
            "    </h1>\r\n",
            "    <p>\r\n",
            "      <br>\r\n",
            "    </p>\r\n",
            "    <h2>\r\n",
            "      By Herman Melville\r\n",
            "    </h2>\r\n",
            "    <p>\r\n",
            "      <br> <br>\r\n",
            "    </p>\r\n",
            "    <hr>\r\n",
            "    <p>\r\n",
            "      <br> <br>\r\n",
            "    </p>\r\n",
            "    <blockquote>\r\n",
            "      <p class=\"toc\" style=\"font-size: x-large;\">\r\n",
            "        <b>CONTENTS</b>\r\n",
            "      </p>\r\n",
            "      <p>\r\n",
            "        <br>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2H_4_0002\"> ETYMOLOGY. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2H_4_0003\"> EXTRACTS (Supplied by a Sub-Sub-Librarian).\r\n",
            "        </a>\r\n",
            "      </p>\r\n",
            "      <p>\r\n",
            "        <br>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0001\"> CHAPTER 1. Loomings. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0002\"> CHAPTER 2. The Carpet-Bag. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#link2HCH0003\"> CHAPTER 3. The Spouter-Inn. </a>\r\n",
            "      </p>\r\n",
            "      <p class=\"toc\">\r\n",
            "        <a href=\"#lin\n"
          ]
        }
      ],
      "source": [
        "#Endereço do texto\n",
        "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
        "\n",
        "r.encoding = 'utf-8'\n",
        "\n",
        "html = r.text\n",
        "\n",
        "print(html[0:2000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir o livro\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "print(text[32000:34000])"
      ],
      "metadata": {
        "id": "wb9wO9ufpCx5",
        "outputId": "d3c750e0-5b05-435f-8994-c61f5e0c7b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h her surf.\r\n",
            "      Right and left, the streets take you waterward. Its extreme downtown is\r\n",
            "      the battery, where that noble mole is washed by waves, and cooled by\r\n",
            "      breezes, which a few hours previous were out of sight of land. Look at the\r\n",
            "      crowds of water-gazers there.\r\n",
            "    \n",
            "\r\n",
            "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\r\n",
            "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\r\n",
            "      you see?—Posted like silent sentinels all around the town, stand\r\n",
            "      thousands upon thousands of mortal men fixed in ocean reveries. Some\r\n",
            "      leaning against the spiles; some seated upon the pier-heads; some looking\r\n",
            "      over the bulwarks of ships from China; some high aloft in the rigging, as\r\n",
            "      if striving to get a still better seaward peep. But these are all\r\n",
            "      landsmen; of week days pent up in lath and plaster—tied to counters,\r\n",
            "      nailed to benches, clinched to desks. How then is this? Are the green\r\n",
            "      fields gone? What do they here?\r\n",
            "    \n",
            "\r\n",
            "      But look! here come more crowds, pacing straight for the water, and\r\n",
            "      seemingly bound for a dive. Strange! Nothing will content them but the\r\n",
            "      extremest limit of the land; loitering under the shady lee of yonder\r\n",
            "      warehouses will not suffice. No. They must get just as nigh the water as\r\n",
            "      they possibly can without falling in. And there they stand—miles of\r\n",
            "      them—leagues. Inlanders all, they come from lanes and alleys,\r\n",
            "      streets and avenues—north, east, south, and west. Yet here they all\r\n",
            "      unite. Tell me, does the magnetic virtue of the needles of the compasses\r\n",
            "      of all those ships attract them thither?\r\n",
            "    \n",
            "\r\n",
            "      Once more. Say you are in the country; in some high land of lakes. Take\r\n",
            "      almost any path you please, and ten to one it carries you down in a dale,\r\n",
            "      and leaves you there by a pool in the stream. There is magic in it. Let\r\n",
            "      the most absent-minded of men be plunged in his deepest r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-Processamento**\n",
        "O resultado de cada uma das etapas a seguir **deve ser utilizada como entrada para as etapas posteriores!!!**"
      ],
      "metadata": {
        "id": "TQ8Uk1P_tkDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicialmente normalize o texto, tornando-o completamente minúsculo. Você pode fazer isso usando a função lower() da classe string.\n",
        "def normalizar(texto):\n",
        "    return texto.lower()\n",
        "\n",
        "texto_normalizado = normalizar(text)\n",
        "print(texto_normalizado[0:50])"
      ],
      "metadata": {
        "id": "SGvDYCC4tng_",
        "outputId": "1afa3a92-ec22-4267-cf8c-bed5f9e6d61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "moby dick; or the whale\n",
            "\n",
            "\n",
            "\n",
            "*** start of the pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize o texto considerando apenas caracteres alpha-numéricos. Você pode fazer isso com expressões regulares e a biblioteca nltk.tokenize.RegexpTokenizer() passando seu padrão ou re.find()\n",
        "def tokenizar(texto):\n",
        "   return nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(texto)\n",
        "\n",
        "\n",
        "tokens = tokenizar(texto_normalizado)\n",
        "\n",
        "print(f\"Número de tokens: {len(tokens)}\")\n",
        "print(\"Primeiros 20 tokens:\", tokens[:20])"
      ],
      "metadata": {
        "id": "rLtNJh41yQk3",
        "outputId": "718c449c-accf-4894-853c-62d7d6f35366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 219426\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente o código para remover tokens que possuem tamanho maior que 15 e menor que 2. Você pode fazer isso usando a função len() para verificar o tamanho das palavras\n",
        "def remove_palavras_pequenas_grandes(tokens):\n",
        "    return [token for token in tokens if 2 <= len(token) <= 15]\n",
        "\n",
        "\n",
        "process_1 = remove_palavras_pequenas_grandes(tokens)\n",
        "print(f\"Número de tokens: {len(process_1)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_1[:20])"
      ],
      "metadata": {
        "id": "CBlxozG5oOsX",
        "outputId": "2042a91f-b14b-45db-db52-3b310ad33956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 210173\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista de tokens sem stopwords. Você pode fazer isso usando a função stopwords.words('english') para encontrar todas as stopwords\n",
        "def remove_stopwords(tokens):\n",
        "  return [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "\n",
        "process_2 = remove_stopwords(process_1)\n",
        "print(f\"Número de tokens: {len(process_2)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_2[:20])"
      ],
      "metadata": {
        "id": "O-b1_sHpoQP2",
        "outputId": "a9cb5818-d6ae-4b44-e630-37f7d283c7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'whale', 'herman', 'melville', 'contents', 'etymology', 'extracts', 'supplied', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens stemmizados. Você pode fazer isso usando a função stem() da classe PorterStemmer e passando para ela a palavra desejada\n",
        "def stemming(tokens):\n",
        "  return [PorterStemmer().stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "process_3 = stemming(process_2)\n",
        "print(f\"Número de tokens: {len(process_3)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_3[:20])"
      ],
      "metadata": {
        "id": "oclfKIqZplCp",
        "outputId": "54dbf0bb-ef4c-4975-d1d3-0972863d945c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens lematizados. Você pode fazer isso usando a função lemmatize() da classe WordNetLemmatizer e passando para ela a palavra desejada\n",
        "def lemmatize(texto):\n",
        "  return [WordNetLemmatizer().lemmatize(token) for token in texto]\n",
        "\n",
        "\n",
        "process_4 = lemmatize(process_3)\n",
        "print(f\"Número de tokens: {len(process_4)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_4[:20])\n",
        "\n",
        "print(text[32000:34000])"
      ],
      "metadata": {
        "id": "ac7r3rursJqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99ffd4a-26f7-4082-a2e8-c14ade35b341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 111191\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n",
            "h her surf.\r\n",
            "      Right and left, the streets take you waterward. Its extreme downtown is\r\n",
            "      the battery, where that noble mole is washed by waves, and cooled by\r\n",
            "      breezes, which a few hours previous were out of sight of land. Look at the\r\n",
            "      crowds of water-gazers there.\r\n",
            "    \n",
            "\r\n",
            "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\r\n",
            "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\r\n",
            "      you see?—Posted like silent sentinels all around the town, stand\r\n",
            "      thousands upon thousands of mortal men fixed in ocean reveries. Some\r\n",
            "      leaning against the spiles; some seated upon the pier-heads; some looking\r\n",
            "      over the bulwarks of ships from China; some high aloft in the rigging, as\r\n",
            "      if striving to get a still better seaward peep. But these are all\r\n",
            "      landsmen; of week days pent up in lath and plaster—tied to counters,\r\n",
            "      nailed to benches, clinched to desks. How then is this? Are the green\r\n",
            "      fields gone? What do they here?\r\n",
            "    \n",
            "\r\n",
            "      But look! here come more crowds, pacing straight for the water, and\r\n",
            "      seemingly bound for a dive. Strange! Nothing will content them but the\r\n",
            "      extremest limit of the land; loitering under the shady lee of yonder\r\n",
            "      warehouses will not suffice. No. They must get just as nigh the water as\r\n",
            "      they possibly can without falling in. And there they stand—miles of\r\n",
            "      them—leagues. Inlanders all, they come from lanes and alleys,\r\n",
            "      streets and avenues—north, east, south, and west. Yet here they all\r\n",
            "      unite. Tell me, does the magnetic virtue of the needles of the compasses\r\n",
            "      of all those ships attract them thither?\r\n",
            "    \n",
            "\r\n",
            "      Once more. Say you are in the country; in some high land of lakes. Take\r\n",
            "      almost any path you please, and ten to one it carries you down in a dale,\r\n",
            "      and leaves you there by a pool in the stream. There is magic in it. Let\r\n",
            "      the most absent-minded of men be plunged in his deepest r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Frequência dos tokens**\n",
        "Mostre a distribuição das palavras para cada uma das etapas de pré-processamento realizadas. Você pode fazer isso utilizando a função nltk.FreqDist() passando sua lista de palavras"
      ],
      "metadata": {
        "id": "Bw7dCTUQBZJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras depois de normalizadas e das pequenas serem removidas\n",
        "def freq_dist(tokens):\n",
        "  return nltk.FreqDist(tokens)\n",
        "\n",
        "print(freq_dist(process_1))"
      ],
      "metadata": {
        "id": "dMq3DqFP71f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3154e287-f03d-4c6b-e58d-56c734e385e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 17095 samples and 210173 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas\n",
        "def freq_dist(tokens):\n",
        "  return nltk.FreqDist(tokens)\n",
        "\n",
        "print(freq_dist(process_2))"
      ],
      "metadata": {
        "id": "4atACCWT76xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1afa40c-4bef-4e9a-e273-db323e656dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 16957 samples and 111191 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas e com stemming\n",
        "def freq_dist(tokens):\n",
        "  return nltk.FreqDist(tokens)\n",
        "\n",
        "print(freq_dist(process_3))"
      ],
      "metadata": {
        "id": "Qf6_YOps78hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a08df7-9aa3-45f8-aad0-faf31f45d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 10689 samples and 111191 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequência das palavras tokenizadas, com stemming e lemming\n",
        "def freq_dist(tokens):\n",
        "  return nltk.FreqDist(tokens)\n",
        "\n",
        "print(freq_dist(process_4))"
      ],
      "metadata": {
        "id": "eY8j-7P28AWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbde070-d2c3-4031-9135-0db39a62a1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 10660 samples and 111191 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perguntas\n",
        "\n",
        "1. **Compare o texto original com o texto após a normalização. Quais diferenças você percebe nas palavras e na estrutura geral?**\n",
        "- Resposta: `O texto normalizado passou por etapas de ajuste para facilitar a tokenização. Tudo foi transformado em letras minúsculas, alguns caracteres foram removidos e ambiguidades foram retiradas.`\n",
        "\n",
        "2. **O que muda na quantidade de tokens ao aplicar a função remove_palavras_pequenas_grandes()? Informe abaixo o total de tokens antes e depois da aplicação da função.**\n",
        "- Resposta: `São removidos os tokens que possuem tamanho maior que 15 e menor que 2`; `Antes: 219426`; `Depois: 210173`\n",
        "\n",
        "3. **Existe diferença entre o pré-processamento antes e depois de aplicar lematização e stemming?**\n",
        "- Resposta: `É importante pré-processar o texto ANTES da lematização e do stemming, removendo ambiguidades, evitando perder contexto e obter erros.`"
      ],
      "metadata": {
        "id": "ngWYeRR8ybtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algoritmo Byte-Pair Encoding:**"
      ],
      "metadata": {
        "id": "V71AONUTvHsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando as três palavras mais frequentes resultantes da lemmatização (process_4), execute, à mão, o algoritmo BPE para k = 3 e insira abaixo o vocabulário e regras obtidas:\n",
        "\n",
        "\n",
        "\n",
        "* palavras mais frequentes: `[('whale', 1646), ('one', 943), ('like', 661)]`\n",
        "* vocabulário obtido: `{'dreadnaught', 'madden', 'twelvemonth', 'acquir', 'zone', 'hath', 'hartshorn', 'amongst', 'resplend', 'jewel', 'magistr', 'adrift', 'certain', 'gem', 'downright', 'post', 'euroclydon', 'prefer', 'aros', 'beelzebub'}`\n",
        "* regras obtidas: `{'er': 'er'}`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QEhXHfgEvOAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distância de Edição**\n",
        "Calcule à mão a distância de edição entre as palavras 'while' e 'like' supondo que inserções ou deleções tem custo 1 e uma substituição tem custo 2 quando os caracteres são diferentes e custo 0 quando são iguais.\n",
        "\n",
        "\n",
        "Insira o valor obtido aqui: `5`\n"
      ],
      "metadata": {
        "id": "OL7AUQkc_qau"
      }
    }
  ]
}